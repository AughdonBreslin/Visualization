<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Supervised is Unsupervised — Learn p(x,y) then infer p(y|x)</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <link rel="stylesheet" href="base.css">
    <link rel="stylesheet" href="visualizer.css">
    <link rel="stylesheet" href="article.css">
    <script defer src="supervised_is_unsupervised.js"></script>
</head>

<body>
    <div class="container article">
        <header>
            <h1>Learning p(x, y) → Inference p(y | x)</h1>
            <div class="subtitle">Step-through demo: estimate class-conditional densities with KDE and compute
                posteriors</div>
            <div class="home-link"><a href="index.html">← Home</a></div>
        </header>

        <main class="article-body">
            <section class="dataset-panel">
                <h2>Dataset</h2>
                <p>Paste CSV (x1,x2,y) or upload a CSV.</p>
                <textarea id="csvInput" rows="6">x1,x2,y
1,2,0
2,1,0
1.5,1.8,0
2,2,0
4,4,1
5,4,1
4.5,5,1
5,5,1
</textarea>
                <div class="dataset-controls">
                    <div class="load-row">
                        <button id="loadCsv">Load CSV</button>
                        <input id="fileInput" type="file" accept="text/csv" />
                    </div>
                </div>
            </section>

            <!-- Calculations: priors and multivariate Gaussian parameter estimates -->
            <section id="calculations-panel" class="calculations-panel">
                <h2>Calculations: Priors & Gaussian parameters</h2>
                <div class="calc-row">
                    <div class="calc-action">
                        <button id="fitGDA">Fit GDA (compute μ, Σ)</button>
                    </div>
                    <div id="gdaParams" class="gda-params">
                        <h3>Fitted parameters</h3>
                        <div id="priors">P(y=0)=– P(y=1)=–</div>
                        <div id="mu0">μ0 = –</div>
                        <div id="sigma0">Σ0 = –</div>
                        <div id="mu1">μ1 = –</div>
                        <div id="sigma1">Σ1 = –</div>
                    </div>
                </div>
                <div class="calc-explain">
                    <h4>Data and GDA formulas</h4>
                    <p>Class priors are computed as p(y=k) = (# samples with y=k) / (total samples).</p>
                    <p>GDA parameter estimates (per-class):</p>
                    <div class="formula">$$\mu_k = \frac{1}{N_k}\sum_{i:\,y_i=k} x_i \\
                        \Sigma_k = \frac{1}{N_k}\sum_{i:\,y_i=k} (x_i - \mu_k)(x_i - \mu_k)^T$$</div>
                    <p>Multivariate normal density used by GDA:</p>
                    <div class="formula">$$\mathcal{N}(x;\mu,\Sigma) =
                        \frac{1}{2\pi\sqrt{|\Sigma|}}\,\exp\left(-\tfrac12 (x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$</div>
                    <div class="theory-block">
                        <h4>Why this generative approach works</h4>
                        <p>Instead of learning a direct map x → y, we model the joint distribution
                            \(p(x,y)=p(y)p(x|y)\). Once we have estimates
                            for the class priors \(p(y)\) and class-conditional densities \(p(x|y)\), we apply Bayes'
                            rule to get the posterior
                            \(p(y|x)=\dfrac{p(y)p(x|y)}{\sum_j p(y=j)p(x|y=j)}\), which is exactly what a classifier
                            needs to make probabilistic predictions.</p>
                        <p>This generative strategy has practical benefits: it yields uncertainty estimates, handles
                            missing/partial features naturally, and
                            can be used to sample data. For teaching and diagnostics it also lets us inspect how the
                            model understands each class.</p>
                    </div>
                </div>
            </section>

            <!-- Visuals and explanations for KDE and GDA -->
            <section class="viz-explain-panel">
                <h2>Visuals: KDE & GDA</h2>
                <div class="viz-row">
                    <div class="viz-panel">
                        <div id="viz" class="viz"></div>
                        <div class="help-text">Click on the plot to query p(y=1 | x) at that point.</div>
                        <div class="info-row">
                            <div id="queryInfo">No query yet</div>
                            <div id="classPriors"></div>
                        </div>
                    </div>
                    <aside class="viz-controls">
                        <h3>Overlays & Options</h3>
                        <div class="options-group">
                            <label><input type="checkbox" id="showKDE" checked /> Show KDE heatmap</label>
                            <label><input type="checkbox" id="showGDA" /> Show GDA ellipses & decision</label>
                        </div>
                        <div class="bandwidth-row">
                            <label for="bandwidth"><strong>Bandwidth:</strong></label>
                            <input id="bandwidth" type="number" step="0.1" min="0.01" value="0.6" />
                            <p>Bandwidth <code>h</code> controls kernel width: smaller → more detail; larger → smoother.
                            </p>
                        </div>
                    </aside>

                </div>

                <div class="kde-explain">
                    <h4>KDE formula</h4>
                    <p>KDE (Kernel Density Estimation) places Gaussian kernels at each class point and averages them to
                        estimate class-conditional densities.</p>
                    <div class="formula">$$K(x; x_i, h) = \frac{1}{2\pi h^2}
                        \exp\left(-\frac{\|x-x_i\|^2}{2h^2}\right)$$</div>
                </div>

                <div class="theory-block wide">
                    <h4>Intuition for the visual tools</h4>
                    <p><strong>KDE</strong> shows a smoothed, local estimate of density by summing contributions from
                        nearby points — it's useful to visualize how the data is distributed
                        without assuming a specific shape.</p>
                    <p><strong>GDA</strong> fits Gaussian clouds to each class. If the true class-conditional densities
                        are roughly elliptical, GDA will capture them well and
                        produce analytic formulas for p(x|y) and p(y|x). The 1‑sigma ellipses show the main direction
                        and spread.</p>
                    <p><strong>How they work together:</strong> KDE gives a flexible picture of class densities (great
                        for exploratory visualization), while GDA provides
                        a simple analytic model that we can compute with by hand to demonstrate Bayes' rule
                        step-by-step.</p>
                    <p><strong>Tradeoffs & Tips:</strong> KDE is flexible but sensitive to bandwidth — try adjusting
                        <code>h</code> and observe over/under-smoothing. GDA is robust and
                        interpretable but can fail if class shapes are non-Gaussian; in practice mixtures of Gaussians
                        or other generative models bridge the gap.</p>
                    <ul>
                        <li><strong>GDA</strong>: a <em>parametric</em> generative model — fit a Gaussian for each class
                            (\(\mu_k,\ \Sigma_k\)). Fast, interpretable, and gives
                            analytic densities and decision boundaries; works well when class-conditional densities are
                            roughly Gaussian.</li>
                        <li><strong>KDE</strong>: a <em>non-parametric</em> density estimator — place Gaussian kernels
                            at each sample and average. Very flexible
                            (can approximate complex shapes) but sensitive to <em>bandwidth</em> and can be
                            computationally heavier.</li>
                    </ul>
                    <p>In this demo we show both: KDE to visualize flexible class densities and GDA to demonstrate
                        closed-form calculations and the
                        step-by-step arithmetic of Bayes' rule.</p>
                </div>
            </section>

            <section id="posterior-panel" class="posterior-panel">
                <h2>Posterior computation (Bayes rule)</h2>
                <p>For a query point x* we compute numerator and denominator and then the posterior:</p>
                <div class="formula">$$\mathrm{num}_k = p(y=k)\,p(x^*\mid y=k)\\
                    p(y=k\mid x^*) = \dfrac{\mathrm{num}_k}{\sum_j \mathrm{num}_j}$$</div>
                <div class="posterior-actions">
                    <div>
                        <button id="example1">Compute for x*=(2,2)</button>
                        <button id="example2">Compute for x*=(4.8,4.7)</button>
                    </div>
                    <div id="gdaExamples" class="gda-examples">
                        <h3>Example inferences</h3>
                        <div id="exampleOut">No examples computed yet.</div>
                    </div>
                </div>
                <div id="calcSteps" class="calc-steps">
                    <div id="calcPriors">Priors: —</div>
                    <div id="calcMuSigma0">Class 0: μ0 = — ; Σ0 = —</div>
                    <div id="calcMuSigma1">Class 1: μ1 = — ; Σ1 = —</div>
                    <div id="calcPoint">No point computed yet.</div>
                </div>
            </section>


        </main>

        <footer class="footer">
            <p class="footer-tag">Created by Aughdon Breslin</p>
        </footer>
    </div>
</body>

</html>