<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Supervised Demo via Unsupervised Techniques</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <link rel="stylesheet" href="../styles/base.css">
    <link rel="stylesheet" href="../styles/article.css">
    <link rel="stylesheet" href="../styles/generative_classification.css">
    <script defer src="../js/formulas_layout.js"></script>
    <script defer src="../js/generative_classification.js"></script>
</head>

<body>
    <div class="container article generative-classification">
        <header>
            <h1>Supervised Demo via Unsupervised Techniques</h1>
            <div class="subtitle">Using Unsupervised Density Estimation to Solve Supervised Classification</div>
            <div class="home-link"><a href="../index.html">← Home</a></div>
        </header>

        <main class="article-body">
            <section class="demo-overview panel">
                <h2>Overview</h2>
                <p>
                    This demo seeks to illustrate how a supervised learning problem such as learning $p(y|x)$ can be solved by learning the joint distribution $p(x, y)$ using unsupervised density estimation, and then applying Bayes' rule to infer the posterior. We will implement two methods for estimating class-conditional densities $p(x|y)$: Kernel Density Estimation (KDE) and Gaussian Discriminant Analysis (GDA). The demo aims to provide intuition for generative modeling approaches to classification, and how they differ from direct discriminative methods.
                </p>
                <p>
                    In a supervised classification, the goal is to learn the mapping from features $x$ to labels $y$, i.e. the posterior distribution $p(y|x)$ or an equivalent decision rule for classification. This is a conditional probability of how likely each class label $y$ is given the observed features $x$. By definition, $p(y|x) = \frac{p(x,y)}{p(x)}$, so if we can estimate the joint distribution $p(x,y)$ we can compute the posterior likelihood of each class $y$ given the observed $x$ using Bayes' rule.
                </p>
                <p>
                    The joint distribution $p(x,y)$ can be decomposed as $p(x,y) = p(y) p(x|y)$, where $p(y)$ is the class prior, formed by the relative frequencies of each class in the training data, and $p(x|y)$ is the class-conditional, learned using unsupervised density estimation within each class. Once the data is split by class label $y$, we can apply any unsupervised density estimation method to learn $p(x|y)$ for each class separately. In this demo we implement two aforementioned methods: KDE, a non-parametric density estimator that places kernels at each data point and averages them; and GDA, a parametric generative model that fits a Gaussian distribution to each class. The means and covariances estimated by GDA are estimated exactly as in a purely unsupervised Gaussian model, but applied separately to each class subset of the data.
                </p>
            </section>
            <section class="dataset-panel panel">
                <h2>Input Dataset</h2>
                <p>Paste a CSV of points $x$ and their class $y$ following ($x_1,x_2,y$) where $x_1, x_2 \in \mathbb{R}$, and $y \in \{0, 1\}$. Type in your input data or use the preloaded example.</p>
                <textarea id="csvInput" rows="15">x1,x2,y
81.0,85.0,1
71.0,85.0,0
80.0,90.0,1
79.0,84.0,0
72.0,94.0,0
74.0,81.0,0
79.0,78.0,0
86.0,86.0,1
82.0,82.0,0
76.0,89.0,1
85.0,97.0,1
86.0,78.0,1
80.0,84.0,0
86.0,85.0,0
83.0,84.0,1
83.0,92.0,1
76.0,78.0,0
73.0,92.0,1
81.0,85.0,1
74.0,90.0,1
81.0,85.0,0
76.0,85.0,0
78.0,88.0,1
87.0,78.0,0
71.0,85.0,0
80.0,80.0,0
76.0,84.0,0
77.0,84.0,0
79.0,89.0,1
84.0,88.0,1
81.0,83.0,0
77.0,89.0,1
80.0,89.0,1
81.0,86.0,1
82.0,80.0,0
79.0,83.0,0
85.0,83.0,1
81.0,84.0,0
83.0,89.0,1
78.0,86.0,1
72.0,90.0,2
73.0,91.0,2
75.0,89.0,2
75.0,91.0,2
74.0,98.0,2
</textarea>

                <div class="dataset-controls">
                    <div class="load-row">
                        <button id="loadCsv">Load the input data</button>
                        <label for="fileInput">Upload a File</label>
                        <input id="fileInput" name="fileInput" type="file" accept="text/csv" />
                    </div>
                </div>
            </section>

            <section id="calculations-panel" class="calculations-panel panel">
                <h2>Calculations: Priors & Approaches</h2>
                <p>To learn $p(y)$, we compute the priors. A prior is the probability of a class before observing any data. This is computed as the fraction of samples belonging to each class, or:</p>
                <div class="formula">$$p(y=k) = \frac{\text{num samples with y=k}}{\text{total samples}}$$</div>
                <p>To learn $p(x|y)$, we have two approaches.</p>
                <p>1. KDE: For a class $k$, given samples {$x_i, ...$} where $y_i = k$, we estimate the class-conditional density $p(x|y=k)$ using Kernel Density Estimation (KDE). KDE places a Gaussian kernel at each sample point and averages them to form a smooth density estimate. The bandwidth parameter $h$ sets the standard deviation of each Gaussian, controlling how fast each point's influence decays with distance. In other words, the bandwidth sets the smoothness of the estimate. The constant $d$ is the dimensionality of the input space. This estimator looks like:</p>
                <div class="formulas">
                    <div class="formula">$$\hat{p}(x|y=k) = \frac{1}{n_kh^d}\sum_{i:\,y_i=k} K(x; x_i, h)$$</div>
                    <div class="formula">$$K(x; x_i, h) = \frac{1}{(2\pi)^{\frac{d}{2}}}
                        \exp\left(-\frac{\|x-x_i\|^2}{2h^2}\right)$$</div>
                </div>
                <p>Combining these two, we get a uniform mixture of Gaussians where each mean is centered at each data point and each have covariance $h^2I$. Note: KDE is non-parametric as it grows with the number of samples. This approach makes no assumption that the class forms a single cluster.</p>
                <div class="formula">
                    $$\hat{p}(x|y=k) = \frac{1}{n_k}\sum_{i:\,y_i=k}^{n_k} \mathcal{N}(x; x_i, h^2I)$$
                </div>
                <p>2. GDA: For a class $k$, we fit a single multivariate Gaussian distribution to the samples {$x_i, ...$} where $y_i = k$. This involves estimating the mean vector $μ_k$ and covariance matrix $Σ_k$ for each class $k$ using maximum likelihood estimation:</p>
                <div class="calc-explain">
                    <div class="formulas">
                        <div class="formula">$$\mu_k = \frac{1}{N_k}\sum_{i:\,y_i=k} x_i$$</div>
                        <div class="formula">$$\Sigma_k = \frac{1}{N_k}\sum_{i:\,y_i=k} (x_i - \mu_k)(x_i - \mu_k)^T$$</div>
                    </div>
                    <p>For each class $y=k$, GDA assumes that all points of class $k$ are generated from one Gaussian:</p>
                    <div class="formula">$$p(x|y=k) = \mathcal{N}(x; \mu_k, \Sigma_k)$$</div>
                    <p>Multivariate normal density used by GDA:</p>
                    <div class="formula">$$\mathcal{N}(x;\mu,\Sigma) =
                        \frac{1}{(2\pi)^{\frac{d}{2}}\sqrt{|\Sigma|}}\,\exp\left(-\tfrac12 (x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$</div>
                </div>
                <p> After loading data, click "Fit GDA" to compute class priors and Gaussian parameters (mean μ and covariance Σ) for each class.</p>
                <button id="fitGDA">Fit GDA (compute μ, Σ)</button>
                
                <h3>Fitted parameters</h3>

                <div id="gdaParams" class="gda-params">
                    <div id="prior0"></div>
                    <div id="prior1"></div>
                    <div id="mu0"></div>
                    <div id="mu1"></div>
                    <div id="sigma0"></div>
                    <div id="sigma1"></div>
                </div>

            </section>

            <!-- Visuals and explanations for KDE and GDA -->
            <section class="viz-explain-panel panel">
                <h2>Visuals: KDE & GDA</h2>
                <div class="viz-row">
                    <div class="viz-panel">
                        <div id="viz" class="viz"></div>
                        <div class="help-text">Click on the plot to query $p(y=k | x)$ and $p(x|y=k)$ at that point.</div>
                        <div class="info-row">
                            <div id="queryInfo">No query yet</div>
                        </div>
                    </div>
                    <aside class="viz-controls">
                        <h3>Overlays & Options</h3>
                        <div class="options-group">
                            <label><input type="checkbox" id="showKDE" checked /> Show KDE heatmap</label>
                            <label><input type="checkbox" id="showGDA" /> Show GDA ellipses & decision</label>
                            <label><input type="checkbox" id="useLogSpace" /> Use log-space (stabilizes for extreme values)</label>
                        </div>
                        <div class="bandwidth-row">
                            <label for="bandwidth">Bandwidth:</label>
                            <input id="bandwidth" type="number" step="0.1" min="0.01" value="0.6" />
                        </div>
                        <div class="legend-area">
                            <h4 class="legend-title">Legend</h4>
                            <div id="classLegend" class="class-legend">No classes loaded</div>
                        </div>
                    </aside>
                </div>

                <div class="theory-block wide">
                    <h4>Interpreting the results</h4>
                    <p>
                        After loading data and fitting GDA, the visualization panel shows the data points colored by class, along with optional overlays for the KDE density estimates and GDA Gaussian ellipses. The KDE heatmap visualizes the estimated class-conditional densities $p(x|y=k)$ for each class using Kernel Density Estimation. The GDA ellipses represent one standard deviation contours of the fitted Gaussian distributions for each class, illustrating how GDA models the data.
                    </p>
                    <p>
                        Clicking on the plot queries the posterior probabilities $p(y=k|x^*)$ and class-conditional densities $p(x^*|y=k)$ at the selected point $x^*$. The posterior probabilities indicate the likelihood of each class given the observed features, while the class-conditional densities show how likely the features are to appear under each class model.
                    </p>
                    <p>
                        Note: To see the log-space stabilization effect, reduce the bandwidth and click on a point far from the data. With a tiny bandwidth, KDE can produce extremely small likelihoods that underflow to zero in normal space, but remain finite in log-space. GDA can too, though the point would have to be much further away.
                    </p>
                </div>
            </section>

            <section id="posterior-panel" class="posterior-panel panel">
                <h2>Posterior Computation</h2>
                <p>Walking through the calculations for both KDE and GDA to compute the posterior probabilities $p(y|x^*)$ at a query point $x^*$.</p>
                <div class="posterior-actions">
                    <div class="examples">
                        <button id="example1">Compute for x*=(75,89.5)</button>
                        
                        <button id="example2" type="button">
                            <span >
                                <span>Compute for x*= (</span>
                                    <input id="example2x1" type="number" step="0.1" value="83" aria-label="x1" />
                                    <span>,</span>
                                    <input id="example2x2" type="number" step="0.1" value="83" aria-label="x2" />
                                    <span>)</span>
                                </span>
                            </span>
                        </button>
                        </span> or click anywhere on the plot above to query a point.
                    </div>
                </div>
                <div id="calcQueryPoint" class="calc-output">No point computed yet.</div>
                <div id="calcPriors" class="calc-output">Priors: -</div>
                <div id="calcMuSigma0" class="calc-output">Class 0: μ0 = - ; Σ0 = -</div>
                <div id="calcPointKDE" class="calc-output">No point computed yet.</div>
                <div id="calcPointGDA" class="calc-output">No point computed yet.</div>
            </section>

            <section id="difference-panel" class="difference-panel panel">
                <h2>Generative versus Discriminative Models</h2>
                <p>
                    The generative approach learns the data-generating story, typically the joint distribution $p(x,y)$, then classifies by applying Bayes' rule.
                    This differs from the discriminative approach, which directly learns the decision boundary $f(x)\to y$ or posterior $p(y\mid x)$.
                    Other examples of generative approaches than seen here include naive Bayes, linear discriminant analysis (LDA), variational autoencoders (VAEs), and diffusion models.
                    Examples for discriminative approaches include logistic regression, support vector machines (SVMs), and standard neural-network classifiers.

                </p>

                <h4>Practical implications</h4>
                <ul>
                    <li>Assumptions: generative models often impose a distributional shape for $p(x\mid y)$ (e.g., Gaussian in GDA). If that assumption is roughly right, they can work very well; if it's wrong, not as well. Discriminative models don't need a full model of $x$, so they're usually less sensitive to "wrong density shape" assumptions.</li>
                    <li>Data efficiency: with limited labeled data, generative models can perform well because they exploit structure in $p(x\mid y)$, and learning $p(x\mid y)$ + $p(y)$ can be easier than directly learning the boundary. With lots of data, discriminative models often win on pure classification accuracy since they focus directly on optimizing the decision boundary.</li>
                    <li>Extra capabilities: generative models can score plausibility (likelihood), handle missing features more naturally, and can generate/simulate samples. Discriminative models focus on predicting $y$ from $x$.</li>
                    <li>Training objectives:
                        <ul>
                            <li>Generative training fits parameters by maximizing the joint log-likelihood of the labeled data under a model of $p(x,y)$.</li>
                            <div class="formula">$$\max_{\theta}\ \sum_{i=1}^N \log p_{\theta}(x_i, y_i)\ =\ \sum_{i=1}^N \big[\log p_{\theta}(y_i) + \log p_{\theta}(x_i\mid y_i)\big]$$</div>
                            <li>Discriminative training fits parameters by maximizing the conditional log-likelihood of the labeled data under a model of $p(y\mid x)$.</li>
                            <div class="formula">$$\max_{\phi}\ \sum_{i=1}^N \log p_{\phi}(y_i\mid x_i)$$</div>
                        </ul>
                    </li>
                </ul>
            </section>
        </main>

        <footer class="footer">
            <p class="footer-tag">Created by <a href="https://linkedin.com/in/aughdon/">Aughdon Breslin</a></p>
        </footer>
    </div>
</body>

</html>