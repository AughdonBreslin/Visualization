<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Linear Regression Regularization</title>

    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

    <link rel="stylesheet" href="../styles/base.css">
    <link rel="stylesheet" href="../styles/article.css">
    <link rel="stylesheet" href="../styles/regularization.css">

    <script defer src="../js/regularization.js"></script>
</head>

<body>
    <div class="container article regularization">
        <header>
            <h1>Linear Regression Regularization</h1>
            <div class="subtitle">Control, Ridge, Lasso, L2 Penalty (GD), and Weight Decay (GD)</div>
            <div class="home-link"><a href="../index.html">← Home</a></div>
        </header>

        <main class="article-body">
            <section class="panel">
                <h2>Overview</h2>
                <p>
                    This demonstration compares how different regularization schemes affect linear regression fits when the model can overfit (e.g., high-degree polynomial features).
                    You can regenerate synthetic data, change feature degree, and sweep $\lambda$ to see how the fitted function, coefficient magnitudes, and train/test error change.
                </p>
                <div class="formulas">
                    <div class="formula">$$\textbf{Control (OLS): }\min_w\ \frac{1}{N}\lVert Xw - y\rVert^2$$</div>
                    <div class="formula">$$\textbf{Ridge: }\min_w\ \frac{1}{N}\lVert Xw - y\rVert^2 + \lambda\lVert w\rVert^2$$</div>
                </div>
                <div class="formulas">
                    <div class="formula">$$\textbf{Lasso: }\min_w\ \frac{1}{N}\lVert Xw - y\rVert^2 + \lambda\lVert w\rVert_1$$</div>
                    <div class="formula">$$\textbf{Weight decay (decoupled): } w\leftarrow (1-\eta\lambda)w - \eta\nabla_w\,\tfrac{1}{N}\lVert Xw-y\rVert^2$$</div>
                </div>
                <div class="formulas">
                    <div class="formula">$$\textbf{L2 penalty (GD): } w\leftarrow w - \eta\left(\nabla_w\,\tfrac{1}{N}\lVert Xw-y\rVert^2 + 2\lambda w\right)$$</div>
                </div>
                <p class="note">
                    Notes: We do not penalize the intercept term. "Weight decay" is shown as a training update rule (common in deep learning); it is closely related to L2 regularization but not identical for all optimizers.
                    L2 Penalty is Ridge implemented via gradient descent rather than as a closed-form solution.
                </p>
            </section>

            <section class="panel">
                <h2>Visualization</h2>
                <div class="viz-row">
                    <div class="viz-panel">
                        <div id="regViz" class="viz"></div>
                        <div class="help-text">The curves show model predictions on a dense grid; points are the sampled training set.</div>

                        <div class="reg-row">
                            <div class="metrics">
                                <h3 style="margin: 6px 0;">Metrics</h3>
                                <div id="metricsTable" class="metrics-table">—</div>
                            </div>
                            <div class="coeffs">
                                <h3 style="margin: 6px 0;">Coefficients of <span id="coeffTitle">—</span></h3>
                                <div id="coeffInfo" class="coeff-info">—</div>
                                <div id="coefViz" class="coef-viz"></div>
                            </div>
                        </div>
                    </div>

                    <aside class="viz-controls">
                        <h3>Data</h3>
                        <div class="control-grid">
                            <label for="trueFn">True function</label>
                            <select id="trueFn">
                                <option value="linear">Linear</option>
                                <option value="cubic">Cubic</option>
                                <option value="sine" selected>Sine</option>
                            </select>

                            <label for="nTrain">Train points</label>
                            <input id="nTrain" type="number" min="10" max="300" step="5" value="60" />

                            <label for="noise">Noise (σ)</label>
                            <input id="noise" type="number" min="0" max="3" step="0.05" value="0.35" />

                            <label for="seed">Seed</label>
                            <input id="seed" type="number" step="1" value="7" />
                        </div>

                        <h3 style="margin-top: 14px;">Show curves</h3>
                        <div class="options-group">
                            <label><input type="checkbox" id="showOLS" checked /> Control (OLS)</label>
                            <label><input type="checkbox" id="showRidge" checked /> Ridge</label>
                            <label><input type="checkbox" id="showLasso" checked /> Lasso</label>
                            <label><input type="checkbox" id="showL2GD" /> L2 penalty (GD)</label>
                            <label><input type="checkbox" id="showWD" /> Weight decay (GD)</label>
                        </div>

                        <h3 style="margin-top: 14px;">Model</h3>
                        <div class="control-grid">
                            <label for="degree">Polynomial degree</label>
                            <input id="degree" type="number" min="1" max="18" step="1" value="9" />

                            <label for="lambda">Regularization ($\log_{10} λ$)</label>
                            <input id="lambda" type="range" min="-6" max="2" step="0.05" value="-2" />
                            <div id="lambdaValue" class="small" style="grid-column: 1 / -1; opacity: 0.9;">λ = 0.01</div>
                        </div>

                        <h3 style="margin-top: 14px;">Optimizer (GD methods)</h3>
                        <div class="control-grid">
                            <label for="lr">Learning rate (η)</label>
                            <input id="lr" type="number" min="0.0001" max="1" step="0.0005" value="0.03" />

                            <label for="iters">Iterations</label>
                            <input id="iters" type="number" min="10" max="50000" step="50" value="2500" />

                            <label for="lassoIters">Lasso iterations</label>
                            <input id="lassoIters" type="number" min="5" max="2000" step="5" value="250" />
                        </div>

                        <h3 style="margin-top: 14px;">Selected method</h3>
                        <div class="control-grid">
                            <label for="selectedMethod"></label>
                            <select id="selectedMethod">
                                <option value="ols">Control (OLS)</option>
                                <option value="ridge">Ridge (closed-form)</option>
                                <option value="lasso">Lasso (coordinate descent)</option>
                                <option value="l2gd">L2 penalty (gradient descent)</option>
                                <option value="wd">Weight decay (decoupled)</option>
                            </select>
                        </div>
                    </aside>
                </div>
            </section>

            <section class="panel">
                <h2>What to look for</h2>
                <ul>
                    <li>As degree increases, OLS tends to overfit (wiggly curve, large coefficients, worse test error).</li>
                    <li>Ridge typically shrinks coefficients smoothly, reducing variance and improving test error for a wide range of $\lambda$.</li>
                    <li>Lasso can drive some coefficients to exactly 0, acting like a form of feature selection.</li>
                    <li> The GD methods blow up if the regularization (λ) or the learning rate (η) is set too high.</li>
                </ul>
            </section>

            <section class="panel">
                <h2>Why the GD methods blow up</h2>
                <p>Both L2 penalty and weight decay depend λ or η to be within certain bounds for the gradient descent to converge properly.</p>
                <ul>
                    <li>L2 minimizes $J(w) = MSE(w) + \lambda \|w\|_2^2$. Its gradient is $\nabla J(w) = \nabla MSE(w) + 2\lambda w$. When updating $w_{t+1} = w_t - \eta \nabla J(w_t)$ = $w_t - \eta (\nabla MSE(w_t) + 2\lambda w_t)$, if λ or η is too large, the term dominates the update and the training behaves more like $w_{t+1} \approx w_t - 2\eta \lambda w_t$ or $w_{t+1} \approx (1 - 2\eta \lambda) w_t$.</li>
                    <ul>
                        <li>If $2\eta \lambda > 1$, the sign flips on each step.</li>
                        <li>If $1 < 2\eta \lambda < 2$, it will still converge, but will oscillate back and forth while doing so, causing slower convergence and potential instability in loss.</li>
                        <li>If $2\eta \lambda > 2$, this continuous overshooting will diverge rapidly.</li>
                    </ul>
                    <li>Weight decay behaves similarly. It minimizes $J(w) = MSE(w) + \lambda \|w\|_2^2$, but the update rule is $w_{t+1} = (1 - \eta \lambda) w_t - \eta \nabla MSE(w_t)$ by construction since we don't want the decay to apply directly to the gradient. When λ or η is too large, we fall into the same $w_{t+1} \approx (1 - \eta \lambda) w_t$ behavior, but this time without the 2.</li>
                    <ul>
                        <li>Note: The 2 is not significant and can be removed from L2 as well by redefining the regularization term as $\frac{\lambda}{2} \|w\|_2^2$.</li>
                    </ul>
                </ul>
            </section>
        </main>

        <footer class="footer">
            <p class="footer-tag">Created by <a href="https://linkedin.com/in/aughdon/">Aughdon Breslin</a></p>
        </footer>
    </div>
</body>

</html>
